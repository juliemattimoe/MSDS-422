{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# Program by Thomas W. Miller, August 16, 2018\n",
    "\n",
    "# Previous work involved gathering embeddings via chakin\n",
    "# Following methods described in\n",
    "#    https://github.com/chakki-works/chakin\n",
    "# The previous program, run-chakin-to-get-embeddings-v001.py\n",
    "# downloaded pre-trained GloVe embeddings, saved them in a zip archive,\n",
    "# and unzipped that archive to create the four word-to-embeddings\n",
    "# text files for use in language models. \n",
    "\n",
    "# This program sets uses word embeddings to set up defaultdict \n",
    "# dictionary data structures, that can them be employed in language\n",
    "# models. This is demonstrated with a simple RNN model for predicting\n",
    "# sentiment (thumbs-down versus thumbs-up) for movie reviews.\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os  # operating system functions\n",
    "import os.path  # for manipulation of file path names\n",
    "\n",
    "import re  # regular expressions\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "RANDOM_SEED = 9999\n",
    "\n",
    "# To make output stable across runs\n",
    "def reset_graph(seed= RANDOM_SEED):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "REMOVE_STOPWORDS = False  # no stopword removal \n",
    "\n",
    "EVOCABSIZE = 10000  # specify desired size of pre-defined embedding vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------- \n",
    "# Select the pre-defined embeddings source        \n",
    "# Define vocabulary size for the language model    \n",
    "# Create a word_to_embedding_dict for GloVe.6B.50d\n",
    "embeddings_directory = 'Desktop/MSDS_422/Assignment_8/embeddings/gloVe.6B'\n",
    "filename = 'glove.6B.50d.txt'\n",
    "embeddings_filename = os.path.join(embeddings_directory, filename)\n",
    "# ------------------------------------------------------------- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading embeddings from Desktop/MSDS_422/Assignment_8/embeddings/gloVe.6B/glove.6B.50d.txt\n",
      "Embedding loaded from disks.\n"
     ]
    }
   ],
   "source": [
    "# Utility function for loading embeddings follows methods described in\n",
    "# https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer\n",
    "# Creates the Python defaultdict dictionary word_to_embedding_dict\n",
    "# for the requested pre-trained word embeddings\n",
    "# \n",
    "# Note the use of defaultdict data structure from the Python Standard Library\n",
    "# collections_defaultdict.py lets the caller specify a default value up front\n",
    "# The default value will be retuned if the key is not a known dictionary key\n",
    "# That is, unknown words are represented by a vector of zeros\n",
    "# For word embeddings, this default value is a vector of zeros\n",
    "# Documentation for the Python standard library:\n",
    "#   Hellmann, D. 2017. The Python 3 Standard Library by Example. Boston: \n",
    "#     Addison-Wesley. [ISBN-13: 978-0-13-429105-5]\n",
    "def load_embedding_from_disks(embeddings_filename, with_indexes=True):\n",
    "    \"\"\"\n",
    "    Read a embeddings txt file. If `with_indexes=True`, \n",
    "    we return a tuple of two dictionnaries\n",
    "    `(word_to_index_dict, index_to_embedding_array)`, \n",
    "    otherwise we return only a direct \n",
    "    `word_to_embedding_dict` dictionnary mapping \n",
    "    from a string to a numpy array.\n",
    "    \"\"\"\n",
    "    if with_indexes:\n",
    "        word_to_index_dict = dict()\n",
    "        index_to_embedding_array = []\n",
    "  \n",
    "    else:\n",
    "        word_to_embedding_dict = dict()\n",
    "\n",
    "    with open(embeddings_filename, 'r', encoding='utf-8') as embeddings_file:\n",
    "        for (i, line) in enumerate(embeddings_file):\n",
    "\n",
    "            split = line.split(' ')\n",
    "\n",
    "            word = split[0]\n",
    "\n",
    "            representation = split[1:]\n",
    "            representation = np.array(\n",
    "                [float(val) for val in representation]\n",
    "            )\n",
    "\n",
    "            if with_indexes:\n",
    "                word_to_index_dict[word] = i\n",
    "                index_to_embedding_array.append(representation)\n",
    "            else:\n",
    "                word_to_embedding_dict[word] = representation\n",
    "\n",
    "    # Empty representation for unknown words.\n",
    "    _WORD_NOT_FOUND = [0.0] * len(representation)\n",
    "    if with_indexes:\n",
    "        _LAST_INDEX = i + 1\n",
    "        word_to_index_dict = defaultdict(\n",
    "            lambda: _LAST_INDEX, word_to_index_dict)\n",
    "        index_to_embedding_array = np.array(\n",
    "            index_to_embedding_array + [_WORD_NOT_FOUND])\n",
    "        return word_to_index_dict, index_to_embedding_array\n",
    "    else:\n",
    "        word_to_embedding_dict = defaultdict(lambda: _WORD_NOT_FOUND)\n",
    "        return word_to_embedding_dict\n",
    "\n",
    "print('\\nLoading embeddings from', embeddings_filename)\n",
    "word_to_index, index_to_embedding = \\\n",
    "    load_embedding_from_disks(embeddings_filename, with_indexes=True)\n",
    "print(\"Embedding loaded from disks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding is of shape: (400001, 50)\n",
      "This means (number of words, number of dimensions per word)\n",
      "\n",
      "The first words are words that tend occur more often.\n",
      "Note: for unknown words, the representation is an empty vector,\n",
      "and the index is the last one. The dictionnary has a limit:\n",
      "    A word --> Index in embedding --> Representation\n",
      "    worsdfkljsdf --> 400000 --> [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "    the --> 0 --> [0.418, 0.24968, -0.41242, 0.1217, 0.34527, -0.044457, -0.49688, -0.17862, -0.00066023, -0.6566, 0.27843, -0.14767, -0.55677, 0.14658, -0.0095095, 0.011658, 0.10204, -0.12792, -0.8443, -0.12181, -0.016801, -0.33279, -0.1552, -0.23131, -0.19181, -1.8823, -0.76746, 0.099051, -0.42125, -0.19526, 4.0071, -0.18594, -0.52287, -0.31681, 0.00059213, 0.0074449, 0.17778, -0.15897, 0.012041, -0.054223, -0.29871, -0.15749, -0.34758, -0.045637, -0.44251, 0.18785, 0.0027849, -0.18411, -0.11514, -0.78581]\n"
     ]
    }
   ],
   "source": [
    "# Additional background code from\n",
    "# https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer\n",
    "# shows the general structure of the data structures for word embeddings\n",
    "# This code is modified for our purposes in language modeling \n",
    "vocab_size, embedding_dim = index_to_embedding.shape\n",
    "print(\"Embedding is of shape: {}\".format(index_to_embedding.shape))\n",
    "print(\"This means (number of words, number of dimensions per word)\\n\")\n",
    "print(\"The first words are words that tend occur more often.\")\n",
    "\n",
    "print(\"Note: for unknown words, the representation is an empty vector,\\n\"\n",
    "      \"and the index is the last one. The dictionnary has a limit:\")\n",
    "print(\"    {} --> {} --> {}\".format(\"A word\", \"Index in embedding\", \n",
    "      \"Representation\"))\n",
    "word = \"worsdfkljsdf\"  # a word obviously not in the vocabulary\n",
    "idx = word_to_index[word] # index for word obviously not in the vocabulary\n",
    "complete_vocabulary_size = idx \n",
    "embd = list(np.array(index_to_embedding[idx], dtype=int)) # \"int\" compact print\n",
    "print(\"    {} --> {} --> {}\".format(word, idx, embd))\n",
    "word = \"the\"\n",
    "idx = word_to_index[word]\n",
    "embd = list(index_to_embedding[idx])  # \"int\" for compact print only.\n",
    "print(\"    {} --> {} --> {}\".format(word, idx, embd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test sentence:  The quick brown fox jumps over the lazy dog \n",
      "\n",
      "Test sentence embeddings from complete vocabulary of 400000 words:\n",
      "\n",
      "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      "quick:  [ 0.13967   -0.53798   -0.18047   -0.25142    0.16203   -0.13868\n",
      " -0.24637    0.75111    0.27264    0.61035   -0.82548    0.038647\n",
      " -0.32361    0.30373   -0.14598   -0.23551    0.39267   -1.1287\n",
      " -0.23636   -1.0629     0.046277   0.29143   -0.25819   -0.094902\n",
      "  0.79478   -1.2095    -0.01039   -0.092086   0.84322   -0.11061\n",
      "  3.0096     0.51652   -0.76986    0.51074    0.37508    0.12156\n",
      "  0.082794   0.43605   -0.1584    -0.61048    0.35006    0.52465\n",
      " -0.51747    0.0034705  0.73625    0.16252    0.85279    0.85268\n",
      "  0.57892    0.64483  ]\n",
      "brown:  [-0.88497   0.71685  -0.40379  -0.10698   0.81457   1.0258   -1.2698\n",
      " -0.49382  -0.27839  -0.92251  -0.49409   0.78942  -0.20066  -0.057371\n",
      "  0.060682  0.30746   0.13441  -0.49376  -0.54788  -0.81912  -0.45394\n",
      "  0.52098   1.0325   -0.8584   -0.65848  -1.2736    0.23616   1.0486\n",
      "  0.18442  -0.3901    2.1385   -0.45301  -0.16911  -0.46737   0.15938\n",
      " -0.095071 -0.26512  -0.056479  0.63849  -1.0494    0.037507  0.76434\n",
      " -0.6412   -0.59594   0.46589   0.31494  -0.34072  -0.59167  -0.31057\n",
      "  0.73274 ]\n",
      "fox:  [ 0.44206   0.059552  0.15861   0.92777   0.1876    0.24256  -1.593\n",
      " -0.79847  -0.34099  -0.24021  -0.32756   0.43639  -0.11057   0.50472\n",
      "  0.43853   0.19738  -0.1498   -0.046979 -0.83286   0.39878   0.062174\n",
      "  0.28803   0.79134   0.31798  -0.21933  -1.1015   -0.080309  0.39122\n",
      "  0.19503  -0.5936    1.7921    0.3826   -0.30509  -0.58686  -0.76935\n",
      " -0.61914  -0.61771  -0.68484  -0.67919  -0.74626  -0.036646  0.78251\n",
      " -1.0072   -0.59057  -0.7849   -0.39113  -0.49727  -0.4283   -0.15204\n",
      "  1.5064  ]\n",
      "jumps:  [-0.46105   -0.34219    0.71473   -0.29778    0.28839    0.6248\n",
      "  0.36807   -0.072746   0.60476    0.31463   -0.052247  -0.62302\n",
      " -0.56332    0.7855     0.18116   -0.31698    0.38298   -0.081953\n",
      " -1.3658    -0.78263    0.39804   -0.17001   -0.11926   -0.40146\n",
      "  1.1057    -0.51142   -0.36614    0.22177    0.34626   -0.30648\n",
      "  1.3869     0.77328    0.5946     1.2577     0.23472   -0.46087\n",
      " -0.009223   0.44534    0.012732  -0.24749   -0.7142     0.02422\n",
      "  0.083527   0.25088   -0.24259   -1.354      1.5481    -0.31728\n",
      "  0.55305   -0.0028062]\n",
      "over:  [ 0.12972    0.088073   0.24375    0.078102  -0.12783    0.27831\n",
      " -0.48693    0.19649   -0.39558   -0.28362   -0.47425   -0.59317\n",
      " -0.58804   -0.31702    0.49593    0.0087594  0.039613  -0.42495\n",
      " -0.97641   -0.46534    0.020675   0.086042   0.39317   -0.51255\n",
      " -0.17913   -1.8333     0.5622     0.41626    0.075127   0.02189\n",
      "  3.784      0.71067   -0.073943   0.15373   -0.3853    -0.070163\n",
      " -0.35374    0.074501  -0.084228  -0.45548   -0.081068   0.39157\n",
      "  0.173      0.2254    -0.12836    0.40951   -0.26079    0.090912\n",
      " -0.60515   -0.9827   ]\n",
      "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      "lazy:  [-0.27611  -0.59712  -0.49227  -1.0372   -0.35878  -0.097425 -0.21014\n",
      " -0.092836 -0.054118  0.4542   -0.53296   0.37602   0.77087   0.79669\n",
      " -0.076608 -0.42515   0.42576   0.32791  -0.21996  -0.20261  -0.85139\n",
      "  0.80547   0.97621   0.9792    1.1118   -0.36062  -0.2588    0.8596\n",
      "  0.73631  -0.18601   1.2376   -0.038938  0.19246   0.52473  -0.04842\n",
      " -0.044149  0.064432  0.087822  0.42232  -0.55991  -0.44096   0.097736\n",
      " -0.17589   1.1799    0.13152  -1.0795    0.45685  -0.63312   1.2752\n",
      "  1.1672  ]\n",
      "dog:  [ 0.11008   -0.38781   -0.57615   -0.27714    0.70521    0.53994\n",
      " -1.0786    -0.40146    1.1504    -0.5678     0.0038977  0.52878\n",
      "  0.64561    0.47262    0.48549   -0.18407    0.1801     0.91397\n",
      " -1.1979    -0.5778    -0.37985    0.33606    0.772      0.75555\n",
      "  0.45506   -1.7671    -1.0503     0.42566    0.41893   -0.68327\n",
      "  1.5673     0.27685   -0.61708    0.64638   -0.076996   0.37118\n",
      "  0.1308    -0.45137    0.25398   -0.74392   -0.086199   0.24068\n",
      " -0.64819    0.83549    1.2502    -0.51379    0.04224   -0.88118\n",
      "  0.7158     0.38519  ]\n"
     ]
    }
   ],
   "source": [
    "# Show how to use embeddings dictionaries with a test sentence\n",
    "# This is a famous typing exercise with all letters of the alphabet\n",
    "# https://en.wikipedia.org/wiki/The_quick_brown_fox_jumps_over_the_lazy_dog\n",
    "a_typing_test_sentence = 'The quick brown fox jumps over the lazy dog'\n",
    "print('\\nTest sentence: ', a_typing_test_sentence, '\\n')\n",
    "words_in_test_sentence = a_typing_test_sentence.split()\n",
    "\n",
    "print('Test sentence embeddings from complete vocabulary of', \n",
    "      complete_vocabulary_size, 'words:\\n')\n",
    "for word in words_in_test_sentence:\n",
    "    word_ = word.lower()\n",
    "    embedding = index_to_embedding[word_to_index[word_]]\n",
    "    print(word_ + \": \", embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test sentence embeddings from vocabulary of 10000 words:\n",
      "\n",
      "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      "quick:  [ 0.13967   -0.53798   -0.18047   -0.25142    0.16203   -0.13868\n",
      " -0.24637    0.75111    0.27264    0.61035   -0.82548    0.038647\n",
      " -0.32361    0.30373   -0.14598   -0.23551    0.39267   -1.1287\n",
      " -0.23636   -1.0629     0.046277   0.29143   -0.25819   -0.094902\n",
      "  0.79478   -1.2095    -0.01039   -0.092086   0.84322   -0.11061\n",
      "  3.0096     0.51652   -0.76986    0.51074    0.37508    0.12156\n",
      "  0.082794   0.43605   -0.1584    -0.61048    0.35006    0.52465\n",
      " -0.51747    0.0034705  0.73625    0.16252    0.85279    0.85268\n",
      "  0.57892    0.64483  ]\n",
      "brown:  [-0.88497   0.71685  -0.40379  -0.10698   0.81457   1.0258   -1.2698\n",
      " -0.49382  -0.27839  -0.92251  -0.49409   0.78942  -0.20066  -0.057371\n",
      "  0.060682  0.30746   0.13441  -0.49376  -0.54788  -0.81912  -0.45394\n",
      "  0.52098   1.0325   -0.8584   -0.65848  -1.2736    0.23616   1.0486\n",
      "  0.18442  -0.3901    2.1385   -0.45301  -0.16911  -0.46737   0.15938\n",
      " -0.095071 -0.26512  -0.056479  0.63849  -1.0494    0.037507  0.76434\n",
      " -0.6412   -0.59594   0.46589   0.31494  -0.34072  -0.59167  -0.31057\n",
      "  0.73274 ]\n",
      "fox:  [ 0.44206   0.059552  0.15861   0.92777   0.1876    0.24256  -1.593\n",
      " -0.79847  -0.34099  -0.24021  -0.32756   0.43639  -0.11057   0.50472\n",
      "  0.43853   0.19738  -0.1498   -0.046979 -0.83286   0.39878   0.062174\n",
      "  0.28803   0.79134   0.31798  -0.21933  -1.1015   -0.080309  0.39122\n",
      "  0.19503  -0.5936    1.7921    0.3826   -0.30509  -0.58686  -0.76935\n",
      " -0.61914  -0.61771  -0.68484  -0.67919  -0.74626  -0.036646  0.78251\n",
      " -1.0072   -0.59057  -0.7849   -0.39113  -0.49727  -0.4283   -0.15204\n",
      "  1.5064  ]\n",
      "jumps:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "over:  [ 0.12972    0.088073   0.24375    0.078102  -0.12783    0.27831\n",
      " -0.48693    0.19649   -0.39558   -0.28362   -0.47425   -0.59317\n",
      " -0.58804   -0.31702    0.49593    0.0087594  0.039613  -0.42495\n",
      " -0.97641   -0.46534    0.020675   0.086042   0.39317   -0.51255\n",
      " -0.17913   -1.8333     0.5622     0.41626    0.075127   0.02189\n",
      "  3.784      0.71067   -0.073943   0.15373   -0.3853    -0.070163\n",
      " -0.35374    0.074501  -0.084228  -0.45548   -0.081068   0.39157\n",
      "  0.173      0.2254    -0.12836    0.40951   -0.26079    0.090912\n",
      " -0.60515   -0.9827   ]\n",
      "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      "lazy:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "dog:  [ 0.11008   -0.38781   -0.57615   -0.27714    0.70521    0.53994\n",
      " -1.0786    -0.40146    1.1504    -0.5678     0.0038977  0.52878\n",
      "  0.64561    0.47262    0.48549   -0.18407    0.1801     0.91397\n",
      " -1.1979    -0.5778    -0.37985    0.33606    0.772      0.75555\n",
      "  0.45506   -1.7671    -1.0503     0.42566    0.41893   -0.68327\n",
      "  1.5673     0.27685   -0.61708    0.64638   -0.076996   0.37118\n",
      "  0.1308    -0.45137    0.25398   -0.74392   -0.086199   0.24068\n",
      " -0.64819    0.83549    1.2502    -0.51379    0.04224   -0.88118\n",
      "  0.7158     0.38519  ]\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------- \n",
    "# Define vocabulary size for the language model    \n",
    "# To reduce the size of the vocabulary to the n most frequently used words\n",
    "\n",
    "def default_factory():\n",
    "    return EVOCABSIZE  # last/unknown-word row in limited_index_to_embedding\n",
    "# dictionary has the items() function, returns list of (key, value) tuples\n",
    "limited_word_to_index = defaultdict(default_factory, \\\n",
    "    {k: v for k, v in word_to_index.items() if v < EVOCABSIZE})\n",
    "\n",
    "# Select the first EVOCABSIZE rows to the index_to_embedding\n",
    "limited_index_to_embedding = index_to_embedding[0:EVOCABSIZE,:]\n",
    "# Set the unknown-word row to be all zeros as previously\n",
    "limited_index_to_embedding = np.append(limited_index_to_embedding, \n",
    "    index_to_embedding[index_to_embedding.shape[0] - 1, :].\\\n",
    "        reshape(1,embedding_dim), \n",
    "    axis = 0)\n",
    "\n",
    "# Delete large numpy array to clear some CPU RAM\n",
    "del index_to_embedding\n",
    "\n",
    "# Verify the new vocabulary: should get same embeddings for test sentence\n",
    "# Note that a small EVOCABSIZE may yield some zero vectors for embeddings\n",
    "print('\\nTest sentence embeddings from vocabulary of', EVOCABSIZE, 'words:\\n')\n",
    "for word in words_in_test_sentence:\n",
    "    word_ = word.lower()\n",
    "    embedding = limited_index_to_embedding[limited_word_to_index[word_]]\n",
    "    print(word_ + \": \", embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# code for working with movie reviews data \n",
    "# Source: Miller, T. W. (2016). Web and Network Data Science.\n",
    "#    Upper Saddle River, N.J.: Pearson Education.\n",
    "#    ISBN-13: 978-0-13-388644-3\n",
    "# This original study used a simple bag-of-words approach\n",
    "# to sentiment analysis, along with pre-defined lists of\n",
    "# negative and positive words.        \n",
    "# Code available at:  https://github.com/mtpa/wnds       \n",
    "# ------------------------------------------------------------\n",
    "# Utility function to get file names within a directory\n",
    "def listdir_no_hidden(path):\n",
    "    start_list = os.listdir(path)\n",
    "    end_list = []\n",
    "    for file in start_list:\n",
    "        if (not file.startswith('.')):\n",
    "            end_list.append(file)\n",
    "    return(end_list)\n",
    "\n",
    "# define list of codes to be dropped from document\n",
    "# carriage-returns, line-feeds, tabs\n",
    "codelist = ['\\r', '\\n', '\\t']   \n",
    "\n",
    "# We will not remove stopwords in this exercise because they are\n",
    "# important to keeping sentences intact\n",
    "if REMOVE_STOPWORDS:\n",
    "    print(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# previous analysis of a list of top terms showed a number of words, along \n",
    "# with contractions and other word strings to drop from further analysis, add\n",
    "# these to the usual English stopwords to be dropped from a document collection\n",
    "    more_stop_words = ['cant','didnt','doesnt','dont','goes','isnt','hes',\\\n",
    "        'shes','thats','theres','theyre','wont','youll','youre','youve', 'br'\\\n",
    "        've', 're', 'vs'] \n",
    "\n",
    "    some_proper_nouns_to_remove = ['dick','ginger','hollywood','jack',\\\n",
    "        'jill','john','karloff','kudrow','orson','peter','tcm','tom',\\\n",
    "        'toni','welles','william','wolheim','nikita']\n",
    "\n",
    "    # start with the initial list and add to it for movie text work \n",
    "    stoplist = nltk.corpus.stopwords.words('english') + more_stop_words +\\\n",
    "        some_proper_nouns_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text parsing function for creating text documents \n",
    "# there is more we could do for data preparation \n",
    "# stemming... looking for contractions... possessives... \n",
    "# but we will work with what we have in this parsing function\n",
    "# if we want to do stemming at a later time, we can use\n",
    "#     porter = nltk.PorterStemmer()  \n",
    "# in a construction like this\n",
    "#     words_stemmed =  [porter.stem(word) for word in initial_words]  \n",
    "def text_parse(string):\n",
    "    # replace non-alphanumeric with space \n",
    "    temp_string = re.sub('[^a-zA-Z]', '  ', string)    \n",
    "    # replace codes with space\n",
    "    for i in range(len(codelist)):\n",
    "        stopstring = ' ' + codelist[i] + '  '\n",
    "        temp_string = re.sub(stopstring, '  ', temp_string)      \n",
    "    # replace single-character words with space\n",
    "    temp_string = re.sub('\\s.\\s', ' ', temp_string)   \n",
    "    # convert uppercase to lowercase\n",
    "    temp_string = temp_string.lower()    \n",
    "    if REMOVE_STOPWORDS:\n",
    "        # replace selected character strings/stop-words with space\n",
    "        for i in range(len(stoplist)):\n",
    "            stopstring = ' ' + str(stoplist[i]) + ' '\n",
    "            temp_string = re.sub(stopstring, ' ', temp_string)        \n",
    "    # replace multiple blank characters with one blank character\n",
    "    temp_string = re.sub('\\s+', ' ', temp_string)    \n",
    "    return(temp_string)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Directory: movie-reviews-negative\n",
      "500 files found\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------\n",
    "# gather data for 500 negative movie reviews\n",
    "# -----------------------------------------------\n",
    "dir_name = 'movie-reviews-negative'\n",
    "    \n",
    "filenames = listdir_no_hidden(path=dir_name)\n",
    "num_files = len(filenames)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
    "    assert file_exists\n",
    "print('\\nDirectory:',dir_name)    \n",
    "print('%d files found' % len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing document files under movie-reviews-negative\n"
     ]
    }
   ],
   "source": [
    "# Read data for negative movie reviews\n",
    "# Data will be stored in a list of lists where the each list represents \n",
    "# a document and document is a list of words.\n",
    "# We then break the text into words.\n",
    "\n",
    "def read_data(filename):\n",
    "\n",
    "  with open(filename, encoding='utf-8') as f:\n",
    "    data = tf.compat.as_str(f.read())\n",
    "    data = data.lower()\n",
    "    data = text_parse(data)\n",
    "    data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
    "\n",
    "  return data\n",
    "\n",
    "negative_documents = []\n",
    "\n",
    "print('\\nProcessing document files under', dir_name)\n",
    "for i in range(num_files):\n",
    "    ## print(' ', filenames[i])\n",
    "\n",
    "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
    "\n",
    "    negative_documents.append(words)\n",
    "    # print('Data size (Characters) (Document %d) %d' %(i,len(words)))\n",
    "    # print('Sample string (Document %d) %s'%(i,words[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Directory: movie-reviews-positive\n",
      "500 files found\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------\n",
    "# gather data for 500 positive movie reviews\n",
    "# -----------------------------------------------\n",
    "dir_name = 'movie-reviews-positive'  \n",
    "filenames = listdir_no_hidden(path=dir_name)\n",
    "num_files = len(filenames)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
    "    assert file_exists\n",
    "print('\\nDirectory:',dir_name)    \n",
    "print('%d files found' % len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing document files under movie-reviews-positive\n"
     ]
    }
   ],
   "source": [
    "# Read data for positive movie reviews\n",
    "# Data will be stored in a list of lists where the each list \n",
    "# represents a document and document is a list of words.\n",
    "# We then break the text into words.\n",
    "\n",
    "def read_data(filename):\n",
    "\n",
    "  with open(filename, encoding='utf-8') as f:\n",
    "    data = tf.compat.as_str(f.read())\n",
    "    data = data.lower()\n",
    "    data = text_parse(data)\n",
    "    data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
    "\n",
    "  return data\n",
    "\n",
    "positive_documents = []\n",
    "\n",
    "print('\\nProcessing document files under', dir_name)\n",
    "for i in range(num_files):\n",
    "    ## print(' ', filenames[i])\n",
    "\n",
    "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
    "\n",
    "    positive_documents.append(words)\n",
    "    # print('Data size (Characters) (Document %d) %d' %(i,len(words)))\n",
    "    # print('Sample string (Document %d) %s'%(i,words[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_review_length: 1052\n",
      "min_review_length: 22\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------\n",
    "# convert positive/negative documents into numpy array\n",
    "# note that reviews vary from 22 to 1052 words   \n",
    "# so we use the first 20 and last 20 words of each review \n",
    "# as our word sequences for analysis\n",
    "# -----------------------------------------------------\n",
    "max_review_length = 0  # initialize\n",
    "for doc in negative_documents:\n",
    "    max_review_length = max(max_review_length, len(doc))    \n",
    "for doc in positive_documents:\n",
    "    max_review_length = max(max_review_length, len(doc)) \n",
    "print('max_review_length:', max_review_length) \n",
    "\n",
    "min_review_length = max_review_length  # initialize\n",
    "for doc in negative_documents:\n",
    "    min_review_length = min(min_review_length, len(doc))    \n",
    "for doc in positive_documents:\n",
    "    min_review_length = min(min_review_length, len(doc)) \n",
    "print('min_review_length:', min_review_length) \n",
    "\n",
    "# construct list of 1000 lists with 40 words in each list\n",
    "from itertools import chain\n",
    "documents = []\n",
    "for doc in negative_documents:\n",
    "    doc_begin = doc[0:20]\n",
    "    doc_end = doc[len(doc) - 20: len(doc)]\n",
    "    documents.append(list(chain(*[doc_begin, doc_end])))    \n",
    "for doc in positive_documents:\n",
    "    doc_begin = doc[0:20]\n",
    "    doc_end = doc[len(doc) - 20: len(doc)]\n",
    "    documents.append(list(chain(*[doc_begin, doc_end])))    \n",
    "\n",
    "# create list of lists of lists for embeddings\n",
    "embeddings = []    \n",
    "for doc in documents:\n",
    "    embedding = []\n",
    "    for word in doc:\n",
    "       embedding.append(limited_index_to_embedding[limited_word_to_index[word]]) \n",
    "    embeddings.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First word in first document: while\n",
      "Embedding for this word:\n",
      " [ 0.1011   -0.16566   0.22035  -0.10629   0.46929   0.37968  -0.62815\n",
      " -0.14385  -0.38333   0.055405  0.23511  -0.20999  -0.55395  -0.38271\n",
      "  0.21008   0.02161  -0.23054  -0.13576  -0.61636  -0.4678    0.25716\n",
      "  0.62309   0.3837   -0.25665   0.09041  -1.5184    0.4762   -0.089573\n",
      "  0.025347 -0.25974   3.6121    0.62788   0.15387  -0.062747  0.28699\n",
      " -0.16471  -0.2079    0.4407    0.065441 -0.10303  -0.15489   0.27352\n",
      "  0.38356  -0.098016  0.10705  -0.083071 -0.27168  -0.49441   0.043538\n",
      " -0.39141 ]\n",
      "Corresponding embedding from embeddings list of list of lists\n",
      " [ 0.1011   -0.16566   0.22035  -0.10629   0.46929   0.37968  -0.62815\n",
      " -0.14385  -0.38333   0.055405  0.23511  -0.20999  -0.55395  -0.38271\n",
      "  0.21008   0.02161  -0.23054  -0.13576  -0.61636  -0.4678    0.25716\n",
      "  0.62309   0.3837   -0.25665   0.09041  -1.5184    0.4762   -0.089573\n",
      "  0.025347 -0.25974   3.6121    0.62788   0.15387  -0.062747  0.28699\n",
      " -0.16471  -0.2079    0.4407    0.065441 -0.10303  -0.15489   0.27352\n",
      "  0.38356  -0.098016  0.10705  -0.083071 -0.27168  -0.49441   0.043538\n",
      " -0.39141 ]\n",
      "First word in first document: officially\n",
      "Embedding for this word:\n",
      " [ 0.13682  -0.10324  -0.10126  -0.13996   0.080166 -0.18858  -0.96708\n",
      " -0.066722 -0.254    -0.61085   0.88298  -0.23186  -0.09482  -0.22099\n",
      "  0.85226   0.47223  -0.73086   0.054607 -0.22859   0.6526    0.05519\n",
      " -0.47021   0.35769   0.18049  -0.23699  -1.3029    0.14341   0.044548\n",
      " -0.70229   0.022042  2.3984   -0.46118  -0.88351  -0.5511   -0.25662\n",
      " -0.56969   1.1733   -0.077844 -0.96175  -0.30038  -0.58143  -0.8909\n",
      " -0.34433  -0.53421  -0.84671   0.03971  -1.0485   -0.12547  -0.072426\n",
      " -0.19364 ]\n",
      "Corresponding embedding from embeddings list of list of lists\n",
      " [ 0.13682  -0.10324  -0.10126  -0.13996   0.080166 -0.18858  -0.96708\n",
      " -0.066722 -0.254    -0.61085   0.88298  -0.23186  -0.09482  -0.22099\n",
      "  0.85226   0.47223  -0.73086   0.054607 -0.22859   0.6526    0.05519\n",
      " -0.47021   0.35769   0.18049  -0.23699  -1.3029    0.14341   0.044548\n",
      " -0.70229   0.022042  2.3984   -0.46118  -0.88351  -0.5511   -0.25662\n",
      " -0.56969   1.1733   -0.077844 -0.96175  -0.30038  -0.58143  -0.8909\n",
      " -0.34433  -0.53421  -0.84671   0.03971  -1.0485   -0.12547  -0.072426\n",
      " -0.19364 ]\n",
      "First word in first document: super\n",
      "Embedding for this word:\n",
      " [-0.59147    0.16468    0.18271    1.4054    -0.23347   -0.2986\n",
      " -0.34696   -0.30997   -0.089015  -0.019025   0.28963    0.46779\n",
      " -0.85615    0.68968    0.52189    0.24809   -0.022432   1.009\n",
      " -2.2903    -0.33961   -0.83609   -0.75197    0.34107    0.31885\n",
      " -0.78405   -1.2021    -0.83693   -0.28469    0.41393    0.0074962\n",
      "  1.7202     1.2959    -0.61426    0.4721     0.71448    0.55194\n",
      "  0.43352    0.35058   -1.0558    -1.2248    -0.14596    0.11694\n",
      " -0.39677    0.13791   -0.03571    1.305     -0.14112   -0.18244\n",
      "  0.22988    0.39888  ]\n",
      "Corresponding embedding from embeddings list of list of lists\n",
      " [-0.59147    0.16468    0.18271    1.4054    -0.23347   -0.2986\n",
      " -0.34696   -0.30997   -0.089015  -0.019025   0.28963    0.46779\n",
      " -0.85615    0.68968    0.52189    0.24809   -0.022432   1.009\n",
      " -2.2903    -0.33961   -0.83609   -0.75197    0.34107    0.31885\n",
      " -0.78405   -1.2021    -0.83693   -0.28469    0.41393    0.0074962\n",
      "  1.7202     1.2959    -0.61426    0.4721     0.71448    0.55194\n",
      "  0.43352    0.35058   -1.0558    -1.2248    -0.14596    0.11694\n",
      " -0.39677    0.13791   -0.03571    1.305     -0.14112   -0.18244\n",
      "  0.22988    0.39888  ]\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------    \n",
    "# Check on the embeddings list of list of lists \n",
    "# -----------------------------------------------------\n",
    "# Show the first word in the first document\n",
    "test_word = documents[0][0]    \n",
    "print('First word in first document:', test_word)    \n",
    "print('Embedding for this word:\\n', \n",
    "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
    "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
    "      embeddings[0][0][:])\n",
    "\n",
    "# Show the seventh word in the tenth document\n",
    "test_word = documents[6][9]    \n",
    "print('First word in first document:', test_word)    \n",
    "print('Embedding for this word:\\n', \n",
    "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
    "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
    "      embeddings[6][9][:])\n",
    "\n",
    "# Show the last word in the last document\n",
    "test_word = documents[999][39]    \n",
    "print('First word in first document:', test_word)    \n",
    "print('Embedding for this word:\\n', \n",
    "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
    "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
    "      embeddings[999][39][:])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------    \n",
    "# Make embeddings a numpy array for use in an RNN \n",
    "# Create training and test sets with Scikit Learn\n",
    "# -----------------------------------------------------\n",
    "embeddings_array = np.array(embeddings)\n",
    "\n",
    "# Define the labels to be used 500 negative (0) and 500 positive (1)\n",
    "thumbs_down_up = np.concatenate((np.zeros((500), dtype = np.int32), \n",
    "                      np.ones((500), dtype = np.int32)), axis = 0)\n",
    "\n",
    "# Scikit Learn for random splitting of the data  \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Random splitting of the data in to training (80%) and test (20%)  \n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(embeddings_array, thumbs_down_up, test_size=0.20, \n",
    "                     random_state = RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL 1 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ---- Epoch  0  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.5 Test accuracy: 0.51\n",
      "\n",
      "  ---- Epoch  1  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.5 Test accuracy: 0.445\n",
      "\n",
      "  ---- Epoch  2  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.5 Test accuracy: 0.495\n",
      "\n",
      "  ---- Epoch  3  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.53 Test accuracy: 0.5\n",
      "\n",
      "  ---- Epoch  4  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.56 Test accuracy: 0.495\n",
      "\n",
      "  ---- Epoch  5  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.56 Test accuracy: 0.515\n",
      "\n",
      "  ---- Epoch  6  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.59 Test accuracy: 0.53\n",
      "\n",
      "  ---- Epoch  7  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.61 Test accuracy: 0.55\n",
      "\n",
      "  ---- Epoch  8  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.65 Test accuracy: 0.575\n",
      "\n",
      "  ---- Epoch  9  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.68 Test accuracy: 0.585\n",
      "\n",
      "  ---- Epoch  10  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.69 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  11  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.7 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  12  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.74 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  13  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.74 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  14  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.74 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  15  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.76 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  16  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  17  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  18  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  19  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  20  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.6\n",
      "\n",
      "  ---- Epoch  21  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  22  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.81 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  23  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  24  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  25  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.81 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  26  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  27  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.82 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  28  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.83 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  29  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.83 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  30  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.83 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  31  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.83 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  32  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.85 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  33  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Train accuracy: 0.85 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  34  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.85 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  35  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.85 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  36  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  37  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  38  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  39  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.85 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  40  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  41  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  42  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  43  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  44  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  45  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.87 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  46  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.88 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  47  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.87 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  48  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  49  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.675\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_steps = embeddings_array.shape[1]  # number of words per document \n",
    "n_inputs = embeddings_array.shape[2]  # dimension of  pre-trained embeddings\n",
    "n_neurons = 20  # analyst specified number of neurons\n",
    "n_outputs = 2  # thumbs-down or thumbs-up\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "logits = tf.layers.dense(states, n_outputs)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                          logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        print('\\n  ---- Epoch ', epoch, ' ----\\n')\n",
    "        for iteration in range(y_train.shape[0] // batch_size):          \n",
    "            X_batch = X_train[iteration*batch_size:(iteration + 1)*batch_size,:]\n",
    "            y_batch = y_train[iteration*batch_size:(iteration + 1)*batch_size]\n",
    "            print('  Batch ', iteration, ' training observations from ',  \n",
    "                  iteration*batch_size, ' to ', (iteration + 1)*batch_size-1,)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        print('\\n  Train accuracy:', acc_train, 'Test accuracy:', acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL 2 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ---- Epoch  0  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.545 Test accuracy: 0.485\n",
      "\n",
      "  ---- Epoch  1  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.485 Test accuracy: 0.5\n",
      "\n",
      "  ---- Epoch  2  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.505 Test accuracy: 0.445\n",
      "\n",
      "  ---- Epoch  3  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.495 Test accuracy: 0.44\n",
      "\n",
      "  ---- Epoch  4  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.51 Test accuracy: 0.48\n",
      "\n",
      "  ---- Epoch  5  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.495 Test accuracy: 0.495\n",
      "\n",
      "  ---- Epoch  6  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.52 Test accuracy: 0.495\n",
      "\n",
      "  ---- Epoch  7  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.53 Test accuracy: 0.515\n",
      "\n",
      "  ---- Epoch  8  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.54 Test accuracy: 0.5\n",
      "\n",
      "  ---- Epoch  9  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.55 Test accuracy: 0.51\n",
      "\n",
      "  ---- Epoch  10  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.55 Test accuracy: 0.52\n",
      "\n",
      "  ---- Epoch  11  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.575 Test accuracy: 0.525\n",
      "\n",
      "  ---- Epoch  12  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.585 Test accuracy: 0.535\n",
      "\n",
      "  ---- Epoch  13  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.6 Test accuracy: 0.555\n",
      "\n",
      "  ---- Epoch  14  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.62 Test accuracy: 0.58\n",
      "\n",
      "  ---- Epoch  15  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.62 Test accuracy: 0.585\n",
      "\n",
      "  ---- Epoch  16  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.65 Test accuracy: 0.6\n",
      "\n",
      "  ---- Epoch  17  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.655 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  18  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.665 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  19  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.67 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  20  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.67 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  21  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.69 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  22  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.69 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  23  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.69 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  24  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.695 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  25  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.71 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  26  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.72 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  27  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.725 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  28  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.725 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  29  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.725 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  30  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.73 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  31  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.735 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  32  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.72 Test accuracy: 0.61\n",
      "\n",
      "  ---- Epoch  33  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.715 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  34  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.715 Test accuracy: 0.61\n",
      "\n",
      "  ---- Epoch  35  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.72 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  36  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.73 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  37  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.74 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  38  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.725 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  39  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.74 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  40  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.745 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  41  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.74 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  42  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.74 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  43  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.745 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  44  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.745 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  45  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.745 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  46  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.74 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  47  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.745 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  48  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.735 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  49  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.745 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  50  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.745 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  51  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.745 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  52  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.75 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  53  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.76 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  54  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.76 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  55  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.765 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  56  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.77 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  57  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.775 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  58  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.775 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  59  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.77 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  60  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.775 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  61  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  62  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  63  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.785 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  64  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.805 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  65  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.805 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  66  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.805 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  67  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.81 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  68  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.825 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  69  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.82 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  70  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.82 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  71  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.82 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  72  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.82 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  73  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.82 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  74  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.82 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  75  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.83 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  76  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.83 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  77  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.83 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  78  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.82 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  79  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.815 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  80  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.825 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  81  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.835 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  82  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.84 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  83  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.815 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  84  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.82 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  85  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.835 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  86  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.835 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  87  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.82 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  88  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.835 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  89  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.84 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  90  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.825 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  91  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.835 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  92  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.835 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  93  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.825 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  94  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.835 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  95  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.835 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  96  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.845 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  97  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.835 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  98  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.845 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  99  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.84 Test accuracy: 0.65\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_steps = embeddings_array.shape[1]  # number of words per document \n",
    "n_inputs = embeddings_array.shape[2]  # dimension of  pre-trained embeddings\n",
    "n_neurons = 20  # analyst specified number of neurons\n",
    "n_outputs = 2  # thumbs-down or thumbs-up\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "logits = tf.layers.dense(states, n_outputs)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                          logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        print('\\n  ---- Epoch ', epoch, ' ----\\n')\n",
    "        for iteration in range(y_train.shape[0] // batch_size):          \n",
    "            X_batch = X_train[iteration*batch_size:(iteration + 1)*batch_size,:]\n",
    "            y_batch = y_train[iteration*batch_size:(iteration + 1)*batch_size]\n",
    "            print('  Batch ', iteration, ' training observations from ',  \n",
    "                  iteration*batch_size, ' to ', (iteration + 1)*batch_size-1,)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        print('\\n  Train accuracy:', acc_train, 'Test accuracy:', acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, here is where we bring in the second embedding in order to make the model more accurate\n",
    "\n",
    "embeddings_directory = 'Desktop/MSDS_422/Assignment_8/embeddings/gloVe.6B'\n",
    "filename = 'glove.6B.100d.txt'\n",
    "embeddings_filename = os.path.join(embeddings_directory, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_from_disks(embeddings_filename, with_indexes=True):\n",
    "    \"\"\"\n",
    "    Read a embeddings txt file. If `with_indexes=True`, \n",
    "    we return a tuple of two dictionnaries\n",
    "    `(word_to_index_dict, index_to_embedding_array)`, \n",
    "    otherwise we return only a direct \n",
    "    `word_to_embedding_dict` dictionnary mapping \n",
    "    from a string to a numpy array.\n",
    "    \"\"\"\n",
    "    if with_indexes:\n",
    "        word_to_index_dict = dict()\n",
    "        index_to_embedding_array = []\n",
    "  \n",
    "    else:\n",
    "        word_to_embedding_dict = dict()\n",
    "\n",
    "    with open(embeddings_filename, 'r', encoding='utf-8') as embeddings_file:\n",
    "        for (i, line) in enumerate(embeddings_file):\n",
    "\n",
    "            split = line.split(' ')\n",
    "\n",
    "            word = split[0]\n",
    "\n",
    "            representation = split[1:]\n",
    "            representation = np.array(\n",
    "                [float(val) for val in representation]\n",
    "            )\n",
    "\n",
    "            if with_indexes:\n",
    "                word_to_index_dict[word] = i\n",
    "                index_to_embedding_array.append(representation)\n",
    "            else:\n",
    "                word_to_embedding_dict[word] = representation\n",
    "\n",
    "    # Empty representation for unknown words.\n",
    "    _WORD_NOT_FOUND = [0.0] * len(representation)\n",
    "    if with_indexes:\n",
    "        _LAST_INDEX = i + 1\n",
    "        word_to_index_dict = defaultdict(\n",
    "            lambda: _LAST_INDEX, word_to_index_dict)\n",
    "        index_to_embedding_array = np.array(\n",
    "            index_to_embedding_array + [_WORD_NOT_FOUND])\n",
    "        return word_to_index_dict, index_to_embedding_array\n",
    "    else:\n",
    "        word_to_embedding_dict = defaultdict(lambda: _WORD_NOT_FOUND)\n",
    "        return word_to_embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading embeddings from Desktop/MSDS_422/Assignment_8/embeddings/gloVe.6B/glove.6B.100d.txt\n",
      "Embedding loaded from disks.\n",
      "Embedding is of shape: (400001, 100)\n",
      "This means (number of words, number of dimensions per word)\n",
      "\n",
      "The first words are words that tend occur more often.\n",
      "Note: for unknown words, the representation is an empty vector,\n",
      "and the index is the last one. The dictionnary has a limit:\n",
      "    A word --> Index in embedding --> Representation\n"
     ]
    }
   ],
   "source": [
    "print('\\nLoading embeddings from', embeddings_filename)\n",
    "word_to_index, index_to_embedding = \\\n",
    "    load_embedding_from_disks(embeddings_filename, with_indexes=True)\n",
    "print(\"Embedding loaded from disks.\")\n",
    "\n",
    "vocab_size, embedding_dim = index_to_embedding.shape\n",
    "print(\"Embedding is of shape: {}\".format(index_to_embedding.shape))\n",
    "print(\"This means (number of words, number of dimensions per word)\\n\")\n",
    "print(\"The first words are words that tend occur more often.\")\n",
    "\n",
    "print(\"Note: for unknown words, the representation is an empty vector,\\n\"\n",
    "      \"and the index is the last one. The dictionnary has a limit:\")\n",
    "print(\"    {} --> {} --> {}\".format(\"A word\", \"Index in embedding\", \n",
    "      \"Representation\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    worsdfkljsdf --> 400000 --> [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "    the --> 0 --> [-0.038194, -0.24487, 0.72812, -0.39961, 0.083172, 0.043953, -0.39141, 0.3344, -0.57545, 0.087459, 0.28787, -0.06731, 0.30906, -0.26384, -0.13231, -0.20757, 0.33395, -0.33848, -0.31743, -0.48336, 0.1464, -0.37304, 0.34577, 0.052041, 0.44946, -0.46971, 0.02628, -0.54155, -0.15518, -0.14107, -0.039722, 0.28277, 0.14393, 0.23464, -0.31021, 0.086173, 0.20397, 0.52624, 0.17164, -0.082378, -0.71787, -0.41531, 0.20335, -0.12763, 0.41367, 0.55187, 0.57908, -0.33477, -0.36559, -0.54857, -0.062892, 0.26584, 0.30205, 0.99775, -0.80481, -3.0243, 0.01254, -0.36942, 2.2167, 0.72201, -0.24978, 0.92136, 0.034514, 0.46745, 1.1079, -0.19358, -0.074575, 0.23353, -0.052062, -0.22044, 0.057162, -0.15806, -0.30798, -0.41625, 0.37972, 0.15006, -0.53212, -0.2055, -1.2526, 0.071624, 0.70565, 0.49744, -0.42063, 0.26148, -1.538, -0.30223, -0.073438, -0.28312, 0.37104, -0.25217, 0.016215, -0.017099, -0.38984, 0.87424, -0.72569, -0.51058, -0.52028, -0.1459, 0.8278, 0.27062]\n",
      "\n",
      "Test sentence:  The quick brown fox jumps over the lazy dog \n",
      "\n",
      "Test sentence embeddings from complete vocabulary of 400000 words:\n",
      "\n",
      "the:  [-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141\n",
      "  0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384\n",
      " -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464\n",
      " -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155\n",
      " -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021\n",
      "  0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531\n",
      "  0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559\n",
      " -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243\n",
      "  0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514\n",
      "  0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044\n",
      "  0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212\n",
      " -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148\n",
      " -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215\n",
      " -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459\n",
      "  0.8278    0.27062 ]\n",
      "quick:  [-0.43146   -0.22037   -0.22684   -0.10215   -0.31863   -0.11809\n",
      " -0.093402  -0.069789  -0.29029   -0.34006    0.099652  -0.059301\n",
      " -0.43764    0.19464    0.36997    0.73648   -0.53429   -0.3469\n",
      " -0.21415    0.62954    0.54868    0.29429   -0.32889   -0.61771\n",
      " -0.039648   0.91639   -0.64046    0.28725    0.095922  -0.38774\n",
      " -0.62958    0.33443   -0.4856    -0.2287     0.84277   -0.2204\n",
      " -0.13264   -0.18188    0.077686   0.080045  -0.018909  -0.26018\n",
      "  0.29542   -0.89173   -0.39373   -0.35662    0.011656  -0.37658\n",
      "  0.64576   -0.86503    0.12615    0.18984   -0.26936    0.56216\n",
      "  0.38218   -2.1389    -0.0096116  0.15041    1.2586    -0.35475\n",
      " -0.33285    0.07292   -0.077262   0.049068   0.90212   -0.27539\n",
      " -0.20839    0.26349   -0.26515   -0.70593   -0.68474    0.38424\n",
      " -0.21889   -0.88545    0.38583    0.26481   -0.7641    -0.037501\n",
      " -0.020606  -0.71318    1.1045     0.0453    -0.41902   -0.47667\n",
      " -1.4088    -0.50376    0.88062    0.0072194 -0.42083   -0.62586\n",
      "  0.59608    0.30444   -0.40999   -0.28204   -0.52321   -0.44695\n",
      "  0.21083   -0.010209   0.0086056  0.63263  ]\n",
      "brown:  [-4.3812e-01 -9.9389e-02 -2.6038e-01 -1.1084e+00  1.0550e-01 -5.4542e-02\n",
      "  4.4868e-01  6.1750e-02 -5.8803e-01 -2.1738e-01 -3.6304e-01 -4.0887e-01\n",
      "  3.7877e-02  8.4201e-01  1.0108e-01 -1.8530e-01  5.0486e-01 -3.4252e-01\n",
      "  2.2516e-01 -2.6942e-02 -4.6399e-01  9.9140e-02  1.9596e-02 -6.7435e-01\n",
      "  6.3123e-01  9.5930e-01  1.6215e-01 -4.3166e-01 -2.6642e-01  1.9136e-01\n",
      "  4.5626e-01  6.8918e-01  3.6808e-01 -2.8273e-01 -4.6525e-01  5.9984e-01\n",
      "  1.5369e-01  8.6585e-01  2.7917e-01  5.8380e-01 -4.6627e-01 -1.3590e+00\n",
      " -1.0387e-01  6.0146e-02 -5.2733e-01  1.3135e-01 -3.3766e-01  1.7893e-01\n",
      "  4.4812e-01 -7.0502e-01  6.3793e-01 -7.9508e-01  1.3176e-01  9.7769e-01\n",
      " -2.3153e-01 -2.6450e+00 -1.1464e-01  2.7907e-01  4.9121e-01  5.1274e-01\n",
      "  7.9559e-04  1.7932e-01 -2.9938e-01 -3.3465e-01  9.9161e-01 -6.0262e-01\n",
      "  7.2080e-01  8.4681e-01 -2.3669e-01  1.3666e-01 -3.5330e-01  3.9442e-01\n",
      " -7.2818e-01  9.1664e-02  3.0441e-01  4.8352e-02 -4.1140e-01  3.4362e-01\n",
      "  1.2569e-01  4.2484e-01  4.5470e-01  1.6292e-01 -1.3630e-01 -2.1827e-01\n",
      " -3.8261e-01 -9.2620e-01  5.1256e-01 -3.5184e-01  1.8316e-01  1.9807e-01\n",
      " -1.9681e-02 -7.2242e-01 -4.3439e-01  1.3449e-01 -8.4339e-01  1.3815e-02\n",
      " -1.1325e+00  1.8143e-01 -1.9537e-01 -3.6954e-01]\n",
      "fox:  [ 0.16917   -0.99783    0.24429   -0.79687    0.036447  -0.56127\n",
      "  0.17305    0.29287   -0.43291   -0.82274   -0.11437   -0.28808\n",
      "  0.20501   -0.4878     0.50534   -0.2117     0.48474    0.20959\n",
      "  0.26642    0.6839    -0.2629     0.14794    0.087969  -0.17349\n",
      "  0.61804    0.63733    0.41145    0.46401   -0.2165     0.5\n",
      "  0.65265    1.0608     0.19275    0.141      0.51356    0.72558\n",
      " -0.044848  -0.35761    0.49862    0.73592   -0.38307    0.12159\n",
      " -0.75345    0.80579   -0.48075   -0.40283   -0.49931   -0.60309\n",
      "  0.26126   -0.24109   -0.55885   -0.10622    0.11289    0.49708\n",
      "  0.015915  -2.452     -0.32529    0.20437    0.55361    0.60879\n",
      " -0.083061   0.60856    0.13958   -0.71847    1.1409     0.023752\n",
      "  0.050995   0.29621   -0.16247    1.1456     0.16929   -0.0042113\n",
      " -0.4026    -0.073144   0.096698  -0.15248   -0.69435    0.28032\n",
      " -1.0238     0.58777   -0.34573   -0.60871    0.1842    -0.18736\n",
      " -0.49948   -0.18095   -0.71161    0.69437    0.37298   -0.308\n",
      "  0.2455    -0.94515    0.20393   -0.14885   -1.1153    -0.52266\n",
      " -0.27841    0.027184   0.39712    0.17933  ]\n",
      "jumps:  [ 0.87831   0.76211   0.24562  -0.05516   0.10355  -0.6789   -0.36757\n",
      "  0.52207  -0.37174  -0.10266   1.0164    0.97297   0.028706  0.22013\n",
      "  0.36371   0.79072  -1.5199    0.72657   0.24994   0.07658   0.79373\n",
      "  0.32268  -0.28497   0.30724   0.25493   0.049801 -0.68182   0.059687\n",
      "  0.40362  -0.73308  -0.5968    0.2901    0.15876   0.070044  0.57204\n",
      "  0.70252  -0.86423  -0.1618   -0.026244  0.19154  -0.14515   0.34694\n",
      " -0.62756   0.15429  -0.56114   0.15854  -0.56041  -0.39705   0.31183\n",
      " -0.19028  -0.53601   0.061462  0.12484   1.3302    0.34361  -1.1603\n",
      "  0.10341   0.33138   0.74712   0.11517   0.17949   0.059578  0.22881\n",
      "  0.52396  -0.43749   0.33677   0.028801 -0.67852   0.21443   0.038026\n",
      " -0.87474  -0.22532   0.020465  1.0772    0.71369  -0.14903  -0.53563\n",
      " -0.049547  0.23989  -0.19058   0.13683   0.29553  -0.20244  -0.40515\n",
      " -0.24246  -1.0324    0.32728  -0.46241   0.27757  -0.23512  -0.23432\n",
      "  0.1031   -0.54905   0.21484  -0.16597  -0.34962  -0.16015  -0.2617\n",
      "  0.41802  -0.055161]\n",
      "over:  [-2.9574e-01  3.5345e-01  6.3326e-01  1.9576e-01 -3.0256e-02  5.4244e-01\n",
      " -2.1091e-01  3.2894e-01 -4.8888e-01  1.8379e-01  2.4242e-01  4.0346e-01\n",
      "  1.1973e-01  1.3143e-02  2.4154e-01 -4.0184e-01  2.2176e-01 -2.7837e-01\n",
      " -4.6930e-01 -5.4899e-02  6.5148e-01  1.5958e-01  5.9556e-01  3.3167e-01\n",
      "  7.2649e-01 -4.3182e-01  1.7208e-01 -1.1584e-02 -2.6389e-01 -2.2073e-01\n",
      " -2.8538e-01  3.5863e-01  2.4592e-01  2.2143e-01 -7.6221e-01  3.9352e-01\n",
      " -2.3915e-02  4.3028e-01 -4.7099e-01  2.5162e-01 -5.9507e-01 -1.0495e+00\n",
      "  1.7973e-01 -3.1621e-01  2.3788e-01 -8.8560e-02  3.4751e-01 -5.5950e-01\n",
      "  1.2997e-01 -7.0101e-01  2.8850e-01  1.8111e-01 -2.3004e-01  2.0682e+00\n",
      " -1.4925e-01 -2.8700e+00 -4.6722e-03 -2.2819e-01  1.6623e+00  6.5951e-01\n",
      "  2.1892e-01  6.3600e-01  1.0332e-01  1.3176e-03  4.4414e-01  2.0222e-01\n",
      "  5.2490e-01  6.4131e-01  2.7416e-01  1.0695e-01 -1.2030e-01  4.7109e-02\n",
      " -5.3503e-01 -4.6869e-01 -7.6050e-02  1.0654e-03 -3.8456e-01 -2.4067e-02\n",
      " -7.5877e-01  5.2622e-01  1.3285e+00 -3.9051e-01 -1.2174e-01  5.1886e-01\n",
      " -1.0374e+00 -3.3789e-01  7.4933e-02  2.0036e-01  2.4703e-02 -2.9090e-01\n",
      " -3.2043e-01  2.0445e-02 -9.9185e-01  1.6802e-02 -6.0819e-01 -2.6601e-01\n",
      " -1.9549e-01  2.3127e-01  9.4771e-01 -9.5560e-02]\n",
      "the:  [-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141\n",
      "  0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384\n",
      " -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464\n",
      " -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155\n",
      " -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021\n",
      "  0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531\n",
      "  0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559\n",
      " -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243\n",
      "  0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514\n",
      "  0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044\n",
      "  0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212\n",
      " -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148\n",
      " -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215\n",
      " -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459\n",
      "  0.8278    0.27062 ]\n",
      "lazy:  [ 0.14481   -0.20397    0.3596    -0.59938   -0.93979    0.59784\n",
      " -0.21619    0.73051   -0.36588   -0.19962    0.14571    0.1642\n",
      "  0.1086    -0.78575    0.53327    0.37127   -0.33013   -0.082276\n",
      "  0.73923    0.86931    0.37934    1.2427    -0.19554   -0.53849\n",
      "  0.20681    0.76727   -0.9714    -0.016255  -0.12529    0.36231\n",
      "  0.13313    0.60993    0.44345   -0.3654     0.22531    0.72985\n",
      " -0.69992    0.14427    0.85324    0.21268   -0.46674    0.25746\n",
      " -0.88493    0.042164  -0.24125   -0.11241   -0.52837    0.38905\n",
      "  0.35523    0.29078   -0.47363   -0.30561    0.072255   0.31778\n",
      " -0.64297   -0.3527     0.49651    0.29722    0.68888   -0.54184\n",
      "  0.04863    0.26221   -0.61438   -0.2591     0.66305    0.25526\n",
      "  0.42406   -0.22196   -0.053041  -0.80721   -0.89748   -0.1165\n",
      "  0.45258    0.24817   -0.14874   -0.20952   -0.58499    0.5573\n",
      "  0.47503   -0.6429    -0.11219    0.2627    -0.4951    -0.0085495\n",
      " -0.86135   -0.21422    0.0086754  0.35554   -0.48077   -0.39897\n",
      " -0.012746   0.13761   -0.20283    0.40565    0.056275  -0.35009\n",
      " -0.745     -0.42987   -0.56238   -0.13433  ]\n",
      "dog:  [ 0.30817    0.30938    0.52803   -0.92543   -0.73671    0.63475\n",
      "  0.44197    0.10262   -0.09142   -0.56607   -0.5327     0.2013\n",
      "  0.7704    -0.13983    0.13727    1.1128     0.89301   -0.17869\n",
      " -0.0019722  0.57289    0.59479    0.50428   -0.28991   -1.3491\n",
      "  0.42756    1.2748    -1.1613    -0.41084    0.042804   0.54866\n",
      "  0.18897    0.3759     0.58035    0.66975    0.81156    0.93864\n",
      " -0.51005   -0.070079   0.82819   -0.35346    0.21086   -0.24412\n",
      " -0.16554   -0.78358   -0.48482    0.38968   -0.86356   -0.016391\n",
      "  0.31984   -0.49246   -0.069363   0.018869  -0.098286   1.3126\n",
      " -0.12116   -1.2399    -0.091429   0.35294    0.64645    0.089642\n",
      "  0.70294    1.1244     0.38639    0.52084    0.98787    0.79952\n",
      " -0.34625    0.14095    0.80167    0.20987   -0.86007   -0.15308\n",
      "  0.074523   0.40816    0.019208   0.51587   -0.34428   -0.24525\n",
      " -0.77984    0.27425    0.22418    0.20164    0.017431  -0.014697\n",
      " -1.0235    -0.39695   -0.0056188  0.30569    0.31748    0.021404\n",
      "  0.11837   -0.11319    0.42456    0.53405   -0.16717   -0.27185\n",
      " -0.6255     0.12883    0.62529   -0.52086  ]\n"
     ]
    }
   ],
   "source": [
    "word = \"worsdfkljsdf\"  # a word obviously not in the vocabulary\n",
    "idx = word_to_index[word] # index for word obviously not in the vocabulary\n",
    "complete_vocabulary_size = idx \n",
    "embd = list(np.array(index_to_embedding[idx], dtype=int)) # \"int\" compact print\n",
    "print(\"    {} --> {} --> {}\".format(word, idx, embd))\n",
    "word = \"the\"\n",
    "idx = word_to_index[word]\n",
    "embd = list(index_to_embedding[idx])  # \"int\" for compact print only.\n",
    "print(\"    {} --> {} --> {}\".format(word, idx, embd))\n",
    "\n",
    "a_typing_test_sentence = 'The quick brown fox jumps over the lazy dog'\n",
    "print('\\nTest sentence: ', a_typing_test_sentence, '\\n')\n",
    "words_in_test_sentence = a_typing_test_sentence.split()\n",
    "\n",
    "print('Test sentence embeddings from complete vocabulary of', \n",
    "      complete_vocabulary_size, 'words:\\n')\n",
    "for word in words_in_test_sentence:\n",
    "    word_ = word.lower()\n",
    "    embedding = index_to_embedding[word_to_index[word_]]\n",
    "    print(word_ + \": \", embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test sentence embeddings from vocabulary of 10000 words:\n",
      "\n",
      "the:  [-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141\n",
      "  0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384\n",
      " -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464\n",
      " -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155\n",
      " -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021\n",
      "  0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531\n",
      "  0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559\n",
      " -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243\n",
      "  0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514\n",
      "  0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044\n",
      "  0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212\n",
      " -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148\n",
      " -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215\n",
      " -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459\n",
      "  0.8278    0.27062 ]\n",
      "quick:  [-0.43146   -0.22037   -0.22684   -0.10215   -0.31863   -0.11809\n",
      " -0.093402  -0.069789  -0.29029   -0.34006    0.099652  -0.059301\n",
      " -0.43764    0.19464    0.36997    0.73648   -0.53429   -0.3469\n",
      " -0.21415    0.62954    0.54868    0.29429   -0.32889   -0.61771\n",
      " -0.039648   0.91639   -0.64046    0.28725    0.095922  -0.38774\n",
      " -0.62958    0.33443   -0.4856    -0.2287     0.84277   -0.2204\n",
      " -0.13264   -0.18188    0.077686   0.080045  -0.018909  -0.26018\n",
      "  0.29542   -0.89173   -0.39373   -0.35662    0.011656  -0.37658\n",
      "  0.64576   -0.86503    0.12615    0.18984   -0.26936    0.56216\n",
      "  0.38218   -2.1389    -0.0096116  0.15041    1.2586    -0.35475\n",
      " -0.33285    0.07292   -0.077262   0.049068   0.90212   -0.27539\n",
      " -0.20839    0.26349   -0.26515   -0.70593   -0.68474    0.38424\n",
      " -0.21889   -0.88545    0.38583    0.26481   -0.7641    -0.037501\n",
      " -0.020606  -0.71318    1.1045     0.0453    -0.41902   -0.47667\n",
      " -1.4088    -0.50376    0.88062    0.0072194 -0.42083   -0.62586\n",
      "  0.59608    0.30444   -0.40999   -0.28204   -0.52321   -0.44695\n",
      "  0.21083   -0.010209   0.0086056  0.63263  ]\n",
      "brown:  [-4.3812e-01 -9.9389e-02 -2.6038e-01 -1.1084e+00  1.0550e-01 -5.4542e-02\n",
      "  4.4868e-01  6.1750e-02 -5.8803e-01 -2.1738e-01 -3.6304e-01 -4.0887e-01\n",
      "  3.7877e-02  8.4201e-01  1.0108e-01 -1.8530e-01  5.0486e-01 -3.4252e-01\n",
      "  2.2516e-01 -2.6942e-02 -4.6399e-01  9.9140e-02  1.9596e-02 -6.7435e-01\n",
      "  6.3123e-01  9.5930e-01  1.6215e-01 -4.3166e-01 -2.6642e-01  1.9136e-01\n",
      "  4.5626e-01  6.8918e-01  3.6808e-01 -2.8273e-01 -4.6525e-01  5.9984e-01\n",
      "  1.5369e-01  8.6585e-01  2.7917e-01  5.8380e-01 -4.6627e-01 -1.3590e+00\n",
      " -1.0387e-01  6.0146e-02 -5.2733e-01  1.3135e-01 -3.3766e-01  1.7893e-01\n",
      "  4.4812e-01 -7.0502e-01  6.3793e-01 -7.9508e-01  1.3176e-01  9.7769e-01\n",
      " -2.3153e-01 -2.6450e+00 -1.1464e-01  2.7907e-01  4.9121e-01  5.1274e-01\n",
      "  7.9559e-04  1.7932e-01 -2.9938e-01 -3.3465e-01  9.9161e-01 -6.0262e-01\n",
      "  7.2080e-01  8.4681e-01 -2.3669e-01  1.3666e-01 -3.5330e-01  3.9442e-01\n",
      " -7.2818e-01  9.1664e-02  3.0441e-01  4.8352e-02 -4.1140e-01  3.4362e-01\n",
      "  1.2569e-01  4.2484e-01  4.5470e-01  1.6292e-01 -1.3630e-01 -2.1827e-01\n",
      " -3.8261e-01 -9.2620e-01  5.1256e-01 -3.5184e-01  1.8316e-01  1.9807e-01\n",
      " -1.9681e-02 -7.2242e-01 -4.3439e-01  1.3449e-01 -8.4339e-01  1.3815e-02\n",
      " -1.1325e+00  1.8143e-01 -1.9537e-01 -3.6954e-01]\n",
      "fox:  [ 0.16917   -0.99783    0.24429   -0.79687    0.036447  -0.56127\n",
      "  0.17305    0.29287   -0.43291   -0.82274   -0.11437   -0.28808\n",
      "  0.20501   -0.4878     0.50534   -0.2117     0.48474    0.20959\n",
      "  0.26642    0.6839    -0.2629     0.14794    0.087969  -0.17349\n",
      "  0.61804    0.63733    0.41145    0.46401   -0.2165     0.5\n",
      "  0.65265    1.0608     0.19275    0.141      0.51356    0.72558\n",
      " -0.044848  -0.35761    0.49862    0.73592   -0.38307    0.12159\n",
      " -0.75345    0.80579   -0.48075   -0.40283   -0.49931   -0.60309\n",
      "  0.26126   -0.24109   -0.55885   -0.10622    0.11289    0.49708\n",
      "  0.015915  -2.452     -0.32529    0.20437    0.55361    0.60879\n",
      " -0.083061   0.60856    0.13958   -0.71847    1.1409     0.023752\n",
      "  0.050995   0.29621   -0.16247    1.1456     0.16929   -0.0042113\n",
      " -0.4026    -0.073144   0.096698  -0.15248   -0.69435    0.28032\n",
      " -1.0238     0.58777   -0.34573   -0.60871    0.1842    -0.18736\n",
      " -0.49948   -0.18095   -0.71161    0.69437    0.37298   -0.308\n",
      "  0.2455    -0.94515    0.20393   -0.14885   -1.1153    -0.52266\n",
      " -0.27841    0.027184   0.39712    0.17933  ]\n",
      "jumps:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "over:  [-2.9574e-01  3.5345e-01  6.3326e-01  1.9576e-01 -3.0256e-02  5.4244e-01\n",
      " -2.1091e-01  3.2894e-01 -4.8888e-01  1.8379e-01  2.4242e-01  4.0346e-01\n",
      "  1.1973e-01  1.3143e-02  2.4154e-01 -4.0184e-01  2.2176e-01 -2.7837e-01\n",
      " -4.6930e-01 -5.4899e-02  6.5148e-01  1.5958e-01  5.9556e-01  3.3167e-01\n",
      "  7.2649e-01 -4.3182e-01  1.7208e-01 -1.1584e-02 -2.6389e-01 -2.2073e-01\n",
      " -2.8538e-01  3.5863e-01  2.4592e-01  2.2143e-01 -7.6221e-01  3.9352e-01\n",
      " -2.3915e-02  4.3028e-01 -4.7099e-01  2.5162e-01 -5.9507e-01 -1.0495e+00\n",
      "  1.7973e-01 -3.1621e-01  2.3788e-01 -8.8560e-02  3.4751e-01 -5.5950e-01\n",
      "  1.2997e-01 -7.0101e-01  2.8850e-01  1.8111e-01 -2.3004e-01  2.0682e+00\n",
      " -1.4925e-01 -2.8700e+00 -4.6722e-03 -2.2819e-01  1.6623e+00  6.5951e-01\n",
      "  2.1892e-01  6.3600e-01  1.0332e-01  1.3176e-03  4.4414e-01  2.0222e-01\n",
      "  5.2490e-01  6.4131e-01  2.7416e-01  1.0695e-01 -1.2030e-01  4.7109e-02\n",
      " -5.3503e-01 -4.6869e-01 -7.6050e-02  1.0654e-03 -3.8456e-01 -2.4067e-02\n",
      " -7.5877e-01  5.2622e-01  1.3285e+00 -3.9051e-01 -1.2174e-01  5.1886e-01\n",
      " -1.0374e+00 -3.3789e-01  7.4933e-02  2.0036e-01  2.4703e-02 -2.9090e-01\n",
      " -3.2043e-01  2.0445e-02 -9.9185e-01  1.6802e-02 -6.0819e-01 -2.6601e-01\n",
      " -1.9549e-01  2.3127e-01  9.4771e-01 -9.5560e-02]\n",
      "the:  [-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141\n",
      "  0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384\n",
      " -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464\n",
      " -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155\n",
      " -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021\n",
      "  0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531\n",
      "  0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559\n",
      " -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243\n",
      "  0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514\n",
      "  0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044\n",
      "  0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212\n",
      " -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148\n",
      " -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215\n",
      " -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459\n",
      "  0.8278    0.27062 ]\n",
      "lazy:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "dog:  [ 0.30817    0.30938    0.52803   -0.92543   -0.73671    0.63475\n",
      "  0.44197    0.10262   -0.09142   -0.56607   -0.5327     0.2013\n",
      "  0.7704    -0.13983    0.13727    1.1128     0.89301   -0.17869\n",
      " -0.0019722  0.57289    0.59479    0.50428   -0.28991   -1.3491\n",
      "  0.42756    1.2748    -1.1613    -0.41084    0.042804   0.54866\n",
      "  0.18897    0.3759     0.58035    0.66975    0.81156    0.93864\n",
      " -0.51005   -0.070079   0.82819   -0.35346    0.21086   -0.24412\n",
      " -0.16554   -0.78358   -0.48482    0.38968   -0.86356   -0.016391\n",
      "  0.31984   -0.49246   -0.069363   0.018869  -0.098286   1.3126\n",
      " -0.12116   -1.2399    -0.091429   0.35294    0.64645    0.089642\n",
      "  0.70294    1.1244     0.38639    0.52084    0.98787    0.79952\n",
      " -0.34625    0.14095    0.80167    0.20987   -0.86007   -0.15308\n",
      "  0.074523   0.40816    0.019208   0.51587   -0.34428   -0.24525\n",
      " -0.77984    0.27425    0.22418    0.20164    0.017431  -0.014697\n",
      " -1.0235    -0.39695   -0.0056188  0.30569    0.31748    0.021404\n",
      "  0.11837   -0.11319    0.42456    0.53405   -0.16717   -0.27185\n",
      " -0.6255     0.12883    0.62529   -0.52086  ]\n"
     ]
    }
   ],
   "source": [
    "def default_factory():\n",
    "    return EVOCABSIZE  # last/unknown-word row in limited_index_to_embedding\n",
    "# dictionary has the items() function, returns list of (key, value) tuples\n",
    "limited_word_to_index = defaultdict(default_factory, \\\n",
    "    {k: v for k, v in word_to_index.items() if v < EVOCABSIZE})\n",
    "\n",
    "# Select the first EVOCABSIZE rows to the index_to_embedding\n",
    "limited_index_to_embedding = index_to_embedding[0:EVOCABSIZE,:]\n",
    "# Set the unknown-word row to be all zeros as previously\n",
    "limited_index_to_embedding = np.append(limited_index_to_embedding, \n",
    "    index_to_embedding[index_to_embedding.shape[0] - 1, :].\\\n",
    "        reshape(1,embedding_dim), \n",
    "    axis = 0)\n",
    "\n",
    "# Delete large numpy array to clear some CPU RAM\n",
    "del index_to_embedding\n",
    "\n",
    "print('\\nTest sentence embeddings from vocabulary of', EVOCABSIZE, 'words:\\n')\n",
    "for word in words_in_test_sentence:\n",
    "    word_ = word.lower()\n",
    "    embedding = limited_index_to_embedding[limited_word_to_index[word_]]\n",
    "    print(word_ + \": \", embedding)\n",
    "\n",
    "def listdir_no_hidden(path):\n",
    "    start_list = os.listdir(path)\n",
    "    end_list = []\n",
    "    for file in start_list:\n",
    "        if (not file.startswith('.')):\n",
    "            end_list.append(file)\n",
    "    return(end_list)\n",
    "\n",
    "codelist = ['\\r', '\\n', '\\t']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REMOVE_STOPWORDS:\n",
    "    print(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "    more_stop_words = ['cant','didnt','doesnt','dont','goes','isnt','hes',\\\n",
    "        'shes','thats','theres','theyre','wont','youll','youre','youve', 'br'\\\n",
    "        've', 're', 'vs'] \n",
    "\n",
    "    some_proper_nouns_to_remove = ['dick','ginger','hollywood','jack',\\\n",
    "        'jill','john','karloff','kudrow','orson','peter','tcm','tom',\\\n",
    "        'toni','welles','william','wolheim','nikita']\n",
    "\n",
    "    # start with the initial list and add to it for movie text work \n",
    "    stoplist = nltk.corpus.stopwords.words('english') + more_stop_words +\\\n",
    "        some_proper_nouns_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_parse(string):\n",
    "    # replace non-alphanumeric with space \n",
    "    temp_string = re.sub('[^a-zA-Z]', '  ', string)    \n",
    "    # replace codes with space\n",
    "    for i in range(len(codelist)):\n",
    "        stopstring = ' ' + codelist[i] + '  '\n",
    "        temp_string = re.sub(stopstring, '  ', temp_string)      \n",
    "    # replace single-character words with space\n",
    "    temp_string = re.sub('\\s.\\s', ' ', temp_string)   \n",
    "    # convert uppercase to lowercase\n",
    "    temp_string = temp_string.lower()    \n",
    "    if REMOVE_STOPWORDS:\n",
    "        # replace selected character strings/stop-words with space\n",
    "        for i in range(len(stoplist)):\n",
    "            stopstring = ' ' + str(stoplist[i]) + ' '\n",
    "            temp_string = re.sub(stopstring, ' ', temp_string)        \n",
    "    # replace multiple blank characters with one blank character\n",
    "    temp_string = re.sub('\\s+', ' ', temp_string)    \n",
    "    return(temp_string)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Directory: movie-reviews-negative\n",
      "500 files found\n"
     ]
    }
   ],
   "source": [
    "# gather data for 500 negative movie reviews\n",
    "# -----------------------------------------------\n",
    "dir_name = 'movie-reviews-negative'\n",
    "    \n",
    "filenames = listdir_no_hidden(path=dir_name)\n",
    "num_files = len(filenames)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
    "    assert file_exists\n",
    "print('\\nDirectory:',dir_name)    \n",
    "print('%d files found' % len(filenames))\n",
    "\n",
    "def read_data(filename):\n",
    "\n",
    "  with open(filename, encoding='utf-8') as f:\n",
    "    data = tf.compat.as_str(f.read())\n",
    "    data = data.lower()\n",
    "    data = text_parse(data)\n",
    "    data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
    "\n",
    "  return data\n",
    "\n",
    "negative_documents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing document files under movie-reviews-negative\n"
     ]
    }
   ],
   "source": [
    "print('\\nProcessing document files under', dir_name)\n",
    "for i in range(num_files):\n",
    "    ## print(' ', filenames[i])\n",
    "\n",
    "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
    "\n",
    "    negative_documents.append(words)\n",
    "    # print('Data size (Characters) (Document %d) %d' %(i,len(words)))\n",
    "    # print('Sample string (Document %d) %s'%(i,words[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Directory: movie-reviews-positive\n",
      "500 files found\n"
     ]
    }
   ],
   "source": [
    "dir_name = 'movie-reviews-positive'  \n",
    "filenames = listdir_no_hidden(path=dir_name)\n",
    "num_files = len(filenames)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
    "    assert file_exists\n",
    "print('\\nDirectory:',dir_name)    \n",
    "print('%d files found' % len(filenames))\n",
    "\n",
    "\n",
    "def read_data(filename):\n",
    "\n",
    "  with open(filename, encoding='utf-8') as f:\n",
    "    data = tf.compat.as_str(f.read())\n",
    "    data = data.lower()\n",
    "    data = text_parse(data)\n",
    "    data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
    "\n",
    "  return data\n",
    "\n",
    "positive_documents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing document files under movie-reviews-positive\n"
     ]
    }
   ],
   "source": [
    "print('\\nProcessing document files under', dir_name)\n",
    "for i in range(num_files):\n",
    "    ## print(' ', filenames[i])\n",
    "\n",
    "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
    "\n",
    "    positive_documents.append(words)\n",
    "    # print('Data size (Characters) (Document %d) %d' %(i,len(words)))\n",
    "    # print('Sample string (Document %d) %s'%(i,words[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_review_length: 1052\n",
      "min_review_length: 22\n"
     ]
    }
   ],
   "source": [
    "max_review_length = 0  # initialize\n",
    "for doc in negative_documents:\n",
    "    max_review_length = max(max_review_length, len(doc))    \n",
    "for doc in positive_documents:\n",
    "    max_review_length = max(max_review_length, len(doc)) \n",
    "print('max_review_length:', max_review_length) \n",
    "\n",
    "min_review_length = max_review_length  # initialize\n",
    "for doc in negative_documents:\n",
    "    min_review_length = min(min_review_length, len(doc))    \n",
    "for doc in positive_documents:\n",
    "    min_review_length = min(min_review_length, len(doc)) \n",
    "print('min_review_length:', min_review_length) \n",
    "\n",
    "# construct list of 1000 lists with 40 words in each list\n",
    "from itertools import chain\n",
    "documents = []\n",
    "for doc in negative_documents:\n",
    "    doc_begin = doc[0:20]\n",
    "    doc_end = doc[len(doc) - 20: len(doc)]\n",
    "    documents.append(list(chain(*[doc_begin, doc_end])))    \n",
    "for doc in positive_documents:\n",
    "    doc_begin = doc[0:20]\n",
    "    doc_end = doc[len(doc) - 20: len(doc)]\n",
    "    documents.append(list(chain(*[doc_begin, doc_end])))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First word in first document: while\n",
      "Embedding for this word:\n",
      " [ 0.094157   0.46457    0.4535    -0.15074    0.27223    0.4545\n",
      " -0.14906    0.15345   -0.061775  -0.080787   0.53914   -0.39179\n",
      "  0.083668  -0.10328    0.27425   -0.80995   -0.11588   -0.32288\n",
      " -0.23434    0.19782    0.47749    0.027463   0.49629    0.41455\n",
      "  0.55198    0.13814   -0.14193   -0.65181   -0.055301  -0.026074\n",
      " -0.26557    0.16076   -0.32292   -0.10203    0.08234    0.13615\n",
      "  0.27754    0.19405   -0.2348    -0.12201   -0.39889   -0.6782\n",
      "  0.42633    0.21963   -0.20309    0.16836    0.013425  -0.35281\n",
      " -0.069011  -0.93563    0.16361   -0.13117    0.099808   1.8998\n",
      " -0.26605   -2.4321    -0.34386   -0.46084    1.3691     0.72702\n",
      " -0.18504    0.18016    0.085648   0.46807    0.12802    0.28034\n",
      "  0.68951    0.36221    0.66845    0.32295   -0.58005   -0.27069\n",
      "  0.15057   -0.46084   -0.21336    0.36952   -0.23539    0.075712\n",
      " -0.71302   -0.27551    0.64845    0.10345   -0.64706    0.29101\n",
      " -1.4154    -0.31586   -0.26086    0.24959   -0.20852   -0.28688\n",
      " -0.075658  -0.63833   -0.0040848  0.21971   -0.91796    0.271\n",
      " -0.30677   -0.23741    0.69147   -0.16581  ]\n",
      "Corresponding embedding from embeddings list of list of lists\n",
      " [ 0.094157   0.46457    0.4535    -0.15074    0.27223    0.4545\n",
      " -0.14906    0.15345   -0.061775  -0.080787   0.53914   -0.39179\n",
      "  0.083668  -0.10328    0.27425   -0.80995   -0.11588   -0.32288\n",
      " -0.23434    0.19782    0.47749    0.027463   0.49629    0.41455\n",
      "  0.55198    0.13814   -0.14193   -0.65181   -0.055301  -0.026074\n",
      " -0.26557    0.16076   -0.32292   -0.10203    0.08234    0.13615\n",
      "  0.27754    0.19405   -0.2348    -0.12201   -0.39889   -0.6782\n",
      "  0.42633    0.21963   -0.20309    0.16836    0.013425  -0.35281\n",
      " -0.069011  -0.93563    0.16361   -0.13117    0.099808   1.8998\n",
      " -0.26605   -2.4321    -0.34386   -0.46084    1.3691     0.72702\n",
      " -0.18504    0.18016    0.085648   0.46807    0.12802    0.28034\n",
      "  0.68951    0.36221    0.66845    0.32295   -0.58005   -0.27069\n",
      "  0.15057   -0.46084   -0.21336    0.36952   -0.23539    0.075712\n",
      " -0.71302   -0.27551    0.64845    0.10345   -0.64706    0.29101\n",
      " -1.4154    -0.31586   -0.26086    0.24959   -0.20852   -0.28688\n",
      " -0.075658  -0.63833   -0.0040848  0.21971   -0.91796    0.271\n",
      " -0.30677   -0.23741    0.69147   -0.16581  ]\n",
      "First word in first document: officially\n",
      "Embedding for this word:\n",
      " [ 0.11767   -0.32512   -0.07548    0.46505    0.47256    0.093893\n",
      "  0.08479    0.65836    0.66529   -0.29622    0.3775     0.12391\n",
      "  1.234     -0.071116   0.39342   -0.38526    1.1495     0.24416\n",
      " -0.46665   -0.21367   -0.05016   -0.12825    0.12548    0.21476\n",
      " -0.34354   -0.60212   -0.13795    0.15271    0.65508    0.13828\n",
      "  0.12955    0.74432   -0.05507    0.60147    0.17148   -0.09224\n",
      " -0.15901   -0.60505   -1.0032    -0.27432   -0.50337    0.15612\n",
      "  0.78492   -0.23229    0.43023   -0.23843    0.23606   -0.17012\n",
      "  0.10606   -0.88536    0.10549   -0.70477    0.42277    0.34044\n",
      " -0.73029   -1.7886    -0.66818   -0.069199   1.3215     0.78232\n",
      " -0.013736  -0.16016   -0.22488   -0.30903    0.50288    0.086763\n",
      " -0.73013    0.46699   -0.1384     0.11128    0.37793   -0.48\n",
      " -0.82666    0.2619    -0.13366   -0.2733    -0.14189    0.4065\n",
      " -0.99753   -0.39425   -0.46005   -0.80167   -0.29615    0.21052\n",
      " -0.62401   -0.38146    0.092096  -0.28126    0.39916    0.30631\n",
      " -0.48616   -0.30304   -0.42661    0.551     -0.53672    0.22815\n",
      " -0.28611    0.12191    0.0059174 -0.23961  ]\n",
      "Corresponding embedding from embeddings list of list of lists\n",
      " [ 0.11767   -0.32512   -0.07548    0.46505    0.47256    0.093893\n",
      "  0.08479    0.65836    0.66529   -0.29622    0.3775     0.12391\n",
      "  1.234     -0.071116   0.39342   -0.38526    1.1495     0.24416\n",
      " -0.46665   -0.21367   -0.05016   -0.12825    0.12548    0.21476\n",
      " -0.34354   -0.60212   -0.13795    0.15271    0.65508    0.13828\n",
      "  0.12955    0.74432   -0.05507    0.60147    0.17148   -0.09224\n",
      " -0.15901   -0.60505   -1.0032    -0.27432   -0.50337    0.15612\n",
      "  0.78492   -0.23229    0.43023   -0.23843    0.23606   -0.17012\n",
      "  0.10606   -0.88536    0.10549   -0.70477    0.42277    0.34044\n",
      " -0.73029   -1.7886    -0.66818   -0.069199   1.3215     0.78232\n",
      " -0.013736  -0.16016   -0.22488   -0.30903    0.50288    0.086763\n",
      " -0.73013    0.46699   -0.1384     0.11128    0.37793   -0.48\n",
      " -0.82666    0.2619    -0.13366   -0.2733    -0.14189    0.4065\n",
      " -0.99753   -0.39425   -0.46005   -0.80167   -0.29615    0.21052\n",
      " -0.62401   -0.38146    0.092096  -0.28126    0.39916    0.30631\n",
      " -0.48616   -0.30304   -0.42661    0.551     -0.53672    0.22815\n",
      " -0.28611    0.12191    0.0059174 -0.23961  ]\n"
     ]
    }
   ],
   "source": [
    "# create list of lists of lists for embeddings\n",
    "embeddings = []    \n",
    "for doc in documents:\n",
    "    embedding = []\n",
    "    for word in doc:\n",
    "       embedding.append(limited_index_to_embedding[limited_word_to_index[word]]) \n",
    "    embeddings.append(embedding)\n",
    "\n",
    "test_word = documents[0][0]    \n",
    "print('First word in first document:', test_word)    \n",
    "print('Embedding for this word:\\n', \n",
    "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
    "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
    "      embeddings[0][0][:])\n",
    "\n",
    "# Show the seventh word in the tenth document\n",
    "test_word = documents[6][9]    \n",
    "print('First word in first document:', test_word)    \n",
    "print('Embedding for this word:\\n', \n",
    "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
    "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
    "      embeddings[6][9][:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First word in first document: super\n",
      "Embedding for this word:\n",
      " [-0.44533  -0.44969   1.2393   -1.0486   -0.36844  -0.1852    0.35571\n",
      "  0.14081  -1.3656   -0.16783   0.58801  -0.95256  -0.49226  -0.39318\n",
      " -0.26359  -0.01618   0.021069  0.31416   0.2168    0.22299   0.33585\n",
      " -0.3546   -0.47979  -0.017641  0.22075   0.20135  -0.019484 -0.087378\n",
      " -0.052559  0.70496  -0.24879   0.12083  -0.40985  -0.16628   1.0288\n",
      "  0.082121 -0.74541   1.2733   -0.63724   0.096175  0.74285  -0.53568\n",
      "  0.23198  -1.1901    0.19239   0.38015   0.2989    0.20202   0.24005\n",
      " -0.59483  -1.0667    0.4612    0.50776   0.48553   0.46278  -2.1189\n",
      " -0.16138   1.0984    0.87567   0.080837 -0.4985    0.30497  -0.15166\n",
      " -0.18904   0.94573   0.38747  -0.49592   0.31328  -0.68862   0.43154\n",
      "  0.1418   -0.7425   -0.054038  1.1463   -0.47824   0.20536  -0.42666\n",
      "  0.097123 -0.49852  -0.098802  0.55622  -0.60768  -0.15279   0.46878\n",
      " -0.66378  -0.30409  -0.18661  -0.73092   0.22037   0.1049    0.23679\n",
      "  0.82143   0.024779 -0.0415   -0.38468  -0.68156  -0.52885   0.65856\n",
      "  0.79881  -0.16019 ]\n",
      "Corresponding embedding from embeddings list of list of lists\n",
      " [-0.44533  -0.44969   1.2393   -1.0486   -0.36844  -0.1852    0.35571\n",
      "  0.14081  -1.3656   -0.16783   0.58801  -0.95256  -0.49226  -0.39318\n",
      " -0.26359  -0.01618   0.021069  0.31416   0.2168    0.22299   0.33585\n",
      " -0.3546   -0.47979  -0.017641  0.22075   0.20135  -0.019484 -0.087378\n",
      " -0.052559  0.70496  -0.24879   0.12083  -0.40985  -0.16628   1.0288\n",
      "  0.082121 -0.74541   1.2733   -0.63724   0.096175  0.74285  -0.53568\n",
      "  0.23198  -1.1901    0.19239   0.38015   0.2989    0.20202   0.24005\n",
      " -0.59483  -1.0667    0.4612    0.50776   0.48553   0.46278  -2.1189\n",
      " -0.16138   1.0984    0.87567   0.080837 -0.4985    0.30497  -0.15166\n",
      " -0.18904   0.94573   0.38747  -0.49592   0.31328  -0.68862   0.43154\n",
      "  0.1418   -0.7425   -0.054038  1.1463   -0.47824   0.20536  -0.42666\n",
      "  0.097123 -0.49852  -0.098802  0.55622  -0.60768  -0.15279   0.46878\n",
      " -0.66378  -0.30409  -0.18661  -0.73092   0.22037   0.1049    0.23679\n",
      "  0.82143   0.024779 -0.0415   -0.38468  -0.68156  -0.52885   0.65856\n",
      "  0.79881  -0.16019 ]\n"
     ]
    }
   ],
   "source": [
    "# Show the last word in the last document\n",
    "test_word = documents[999][39]    \n",
    "print('First word in first document:', test_word)    \n",
    "print('Embedding for this word:\\n', \n",
    "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
    "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
    "      embeddings[999][39][:])        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_array1 = np.array(embeddings)\n",
    "\n",
    "# Define the labels to be used 500 negative (0) and 500 positive (1)\n",
    "thumbs_down_up = np.concatenate((np.zeros((500), dtype = np.int32), \n",
    "                      np.ones((500), dtype = np.int32)), axis = 0)\n",
    "\n",
    "# Scikit Learn for random splitting of the data  \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Random splitting of the data in to training (80%) and test (20%)  \n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(embeddings_array1, thumbs_down_up, test_size=0.20, \n",
    "                     random_state = RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL 3 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ---- Epoch  0  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.46 Test accuracy: 0.52\n",
      "\n",
      "  ---- Epoch  1  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.5 Test accuracy: 0.565\n",
      "\n",
      "  ---- Epoch  2  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.53 Test accuracy: 0.55\n",
      "\n",
      "  ---- Epoch  3  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.58 Test accuracy: 0.57\n",
      "\n",
      "  ---- Epoch  4  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.59 Test accuracy: 0.58\n",
      "\n",
      "  ---- Epoch  5  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.6 Test accuracy: 0.55\n",
      "\n",
      "  ---- Epoch  6  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.61 Test accuracy: 0.565\n",
      "\n",
      "  ---- Epoch  7  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.62 Test accuracy: 0.58\n",
      "\n",
      "  ---- Epoch  8  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.63 Test accuracy: 0.595\n",
      "\n",
      "  ---- Epoch  9  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.65 Test accuracy: 0.585\n",
      "\n",
      "  ---- Epoch  10  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.64 Test accuracy: 0.59\n",
      "\n",
      "  ---- Epoch  11  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.65 Test accuracy: 0.595\n",
      "\n",
      "  ---- Epoch  12  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.67 Test accuracy: 0.6\n",
      "\n",
      "  ---- Epoch  13  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.67 Test accuracy: 0.6\n",
      "\n",
      "  ---- Epoch  14  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.69 Test accuracy: 0.6\n",
      "\n",
      "  ---- Epoch  15  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.72 Test accuracy: 0.6\n",
      "\n",
      "  ---- Epoch  16  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.73 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  17  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.72 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  18  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.74 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  19  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.77 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  20  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  21  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  22  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  23  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  24  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  25  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.81 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  26  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.81 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  27  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.81 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  28  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.82 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  29  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.83 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  30  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.84 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  31  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  32  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.88 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  33  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.88 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  34  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.88 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  35  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.89 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  36  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.88 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  37  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.88 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  38  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.88 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  39  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.89 Test accuracy: 0.695\n",
      "\n",
      "  ---- Epoch  40  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.89 Test accuracy: 0.69\n",
      "\n",
      "  ---- Epoch  41  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.91 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  42  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.92 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  43  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.91 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  44  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.91 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  45  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.92 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  46  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.92 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  47  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.92 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  48  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.92 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  49  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.94 Test accuracy: 0.665\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_steps = embeddings_array1.shape[1]  # number of words per document \n",
    "n_inputs = embeddings_array1.shape[2]  # dimension of  pre-trained embeddings\n",
    "n_neurons = 20  # analyst specified number of neurons\n",
    "n_outputs = 2  # thumbs-down or thumbs-up\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "logits = tf.layers.dense(states, n_outputs)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                          logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        print('\\n  ---- Epoch ', epoch, ' ----\\n')\n",
    "        for iteration in range(y_train.shape[0] // batch_size):          \n",
    "            X_batch = X_train[iteration*batch_size:(iteration + 1)*batch_size,:]\n",
    "            y_batch = y_train[iteration*batch_size:(iteration + 1)*batch_size]\n",
    "            print('  Batch ', iteration, ' training observations from ',  \n",
    "                  iteration*batch_size, ' to ', (iteration + 1)*batch_size-1,)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        print('\\n  Train accuracy:', acc_train, 'Test accuracy:', acc_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ---- Epoch  0  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.485 Test accuracy: 0.52\n",
      "\n",
      "  ---- Epoch  1  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.515 Test accuracy: 0.54\n",
      "\n",
      "  ---- Epoch  2  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.555 Test accuracy: 0.555\n",
      "\n",
      "  ---- Epoch  3  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.55 Test accuracy: 0.565\n",
      "\n",
      "  ---- Epoch  4  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.555 Test accuracy: 0.57\n",
      "\n",
      "  ---- Epoch  5  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.58 Test accuracy: 0.58\n",
      "\n",
      "  ---- Epoch  6  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.6 Test accuracy: 0.57\n",
      "\n",
      "  ---- Epoch  7  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.605 Test accuracy: 0.565\n",
      "\n",
      "  ---- Epoch  8  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.615 Test accuracy: 0.565\n",
      "\n",
      "  ---- Epoch  9  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.64 Test accuracy: 0.585\n",
      "\n",
      "  ---- Epoch  10  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.645 Test accuracy: 0.58\n",
      "\n",
      "  ---- Epoch  11  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.64 Test accuracy: 0.58\n",
      "\n",
      "  ---- Epoch  12  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.655 Test accuracy: 0.585\n",
      "\n",
      "  ---- Epoch  13  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.67 Test accuracy: 0.585\n",
      "\n",
      "  ---- Epoch  14  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.675 Test accuracy: 0.58\n",
      "\n",
      "  ---- Epoch  15  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.675 Test accuracy: 0.585\n",
      "\n",
      "  ---- Epoch  16  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.685 Test accuracy: 0.59\n",
      "\n",
      "  ---- Epoch  17  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.695 Test accuracy: 0.595\n",
      "\n",
      "  ---- Epoch  18  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.69 Test accuracy: 0.59\n",
      "\n",
      "  ---- Epoch  19  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.695 Test accuracy: 0.595\n",
      "\n",
      "  ---- Epoch  20  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.7 Test accuracy: 0.6\n",
      "\n",
      "  ---- Epoch  21  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.71 Test accuracy: 0.595\n",
      "\n",
      "  ---- Epoch  22  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.725 Test accuracy: 0.595\n",
      "\n",
      "  ---- Epoch  23  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.735 Test accuracy: 0.6\n",
      "\n",
      "  ---- Epoch  24  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.735 Test accuracy: 0.61\n",
      "\n",
      "  ---- Epoch  25  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.74 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  26  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.75 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  27  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.755 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  28  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.76 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  29  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.77 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  30  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  31  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  32  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.785 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  33  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.775 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  34  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.77 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  35  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.775 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  36  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.785 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  37  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.785 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  38  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.795 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  39  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.795 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  40  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  41  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.82 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  42  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.825 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  43  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.82 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  44  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.83 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  45  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.84 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  46  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.85 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  47  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.85 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  48  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.705\n",
      "\n",
      "  ---- Epoch  49  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.715\n",
      "\n",
      "  ---- Epoch  50  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.865 Test accuracy: 0.72\n",
      "\n",
      "  ---- Epoch  51  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.865 Test accuracy: 0.72\n",
      "\n",
      "  ---- Epoch  52  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.875 Test accuracy: 0.715\n",
      "\n",
      "  ---- Epoch  53  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.88 Test accuracy: 0.71\n",
      "\n",
      "  ---- Epoch  54  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.88 Test accuracy: 0.71\n",
      "\n",
      "  ---- Epoch  55  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.88 Test accuracy: 0.71\n",
      "\n",
      "  ---- Epoch  56  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.88 Test accuracy: 0.7\n",
      "\n",
      "  ---- Epoch  57  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.88 Test accuracy: 0.7\n",
      "\n",
      "  ---- Epoch  58  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.885 Test accuracy: 0.7\n",
      "\n",
      "  ---- Epoch  59  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.885 Test accuracy: 0.69\n",
      "\n",
      "  ---- Epoch  60  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.895 Test accuracy: 0.69\n",
      "\n",
      "  ---- Epoch  61  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.895 Test accuracy: 0.69\n",
      "\n",
      "  ---- Epoch  62  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.905 Test accuracy: 0.69\n",
      "\n",
      "  ---- Epoch  63  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.9 Test accuracy: 0.695\n",
      "\n",
      "  ---- Epoch  64  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.895 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  65  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.92 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  66  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.91 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  67  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.915 Test accuracy: 0.69\n",
      "\n",
      "  ---- Epoch  68  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.925 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  69  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.925 Test accuracy: 0.69\n",
      "\n",
      "  ---- Epoch  70  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.93 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  71  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.945 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  72  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.935 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  73  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.94 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  74  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.94 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  75  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.94 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  76  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.94 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  77  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.96 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  78  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.95 Test accuracy: 0.69\n",
      "\n",
      "  ---- Epoch  79  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.955 Test accuracy: 0.69\n",
      "\n",
      "  ---- Epoch  80  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.955 Test accuracy: 0.69\n",
      "\n",
      "  ---- Epoch  81  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.955 Test accuracy: 0.69\n",
      "\n",
      "  ---- Epoch  82  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.955 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  83  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.955 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  84  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.955 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  85  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.96 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  86  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.96 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  87  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.965 Test accuracy: 0.69\n",
      "\n",
      "  ---- Epoch  88  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.97 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  89  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.97 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  90  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  91  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  92  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.97 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  93  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.935 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  94  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.965 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  95  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.985 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  96  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  97  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.955 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  98  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.965 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  99  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  199\n",
      "  Batch  1  training observations from  200  to  399\n",
      "  Batch  2  training observations from  400  to  599\n",
      "  Batch  3  training observations from  600  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.655\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_steps = embeddings_array1.shape[1]  # number of words per document \n",
    "n_inputs = embeddings_array1.shape[2]  # dimension of  pre-trained embeddings\n",
    "n_neurons = 20  # analyst specified number of neurons\n",
    "n_outputs = 2  # thumbs-down or thumbs-up\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "logits = tf.layers.dense(states, n_outputs)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                          logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        print('\\n  ---- Epoch ', epoch, ' ----\\n')\n",
    "        for iteration in range(y_train.shape[0] // batch_size):          \n",
    "            X_batch = X_train[iteration*batch_size:(iteration + 1)*batch_size,:]\n",
    "            y_batch = y_train[iteration*batch_size:(iteration + 1)*batch_size]\n",
    "            print('  Batch ', iteration, ' training observations from ',  \n",
    "                  iteration*batch_size, ' to ', (iteration + 1)*batch_size-1,)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        print('\\n  Train accuracy:', acc_train, 'Test accuracy:', acc_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
